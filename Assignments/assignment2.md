# Assignment 2: Usability Testing

## Introduction
The Frick Collection Virtual Tour allows the public to tour the interior of the Frick Collection, a museum and research center that houses Old Master paintings and European art. It showcases the museum's art collection through panoramic photos, which the user can navigate through using a control panel. To evaluate how users interact with this site, I conducted a usability test on the Frick Collection Virtual Tour.

Usability testing is a way to gather empirical data from your users about how usable your product is. 
This testing can be done early on during the product development as a formative assessment or later after the product launches as validation.
Testing early on can eliminate usability issues before the product is launched, saving time, money, and effort in the long run.
Testing later on to validate the product can help ensure that it is up to standards. 
Regardless of when it is done, usability testing gives insight on how real users interact with your product and what usability issues they encounter.

To conduct usability testing, you need:

* a location (lab, cafe, etc - anywhere you and the user can sit and use the product)
* people (a moderator is optional, a user is required!)
* a product to test
* a questionnaire

The questionnaire typically consists of realistic tasks that users might do while using your product. These tasks should be outlined very concretely by the moderator, so that they can clearly assess whether the user was able to complete it or not.

Besides giving users realistic scenarios in which to use the product, the researcher can include Likert scale measures that capture more subjective aspects of the user experience, such as likelihood that the user would continue to use the product.

Before I do a full usability test with this website, I conducted this pilot test to evaluate whether I have chosen suitable tasks and questions that address the usability issues of interest.

## Methodology
For the pilot test, I used a live version of the Frick Collection Virtual Tour site located on my laptop. ActivePresenter was to record the user and their interaction with the website. Because of limited access to formal testing spaces, I set up a minimalist portable test lab in my apartment.

The session lasted around 16 minutes, and the participant completed tasks relevant to the usability issues identified in a heuristic evaluation.

In the heuristic evaluation, I identified three major issues with the virtual tour experience:

1. Users are unable to see what pieces or collections are held in each room of the museum from the virtual tour screen
2. The map is not readily accessible from the virtual tour screen
3. Users are unable to view an art piece in high resolution or full screen

With these issues in mind, I crafted three scenarios to test with usability testing:

1. Finding a specific painting by name
2. "Walking" through the virtual museum by traveling from room to room
3. Viewing multiple paintings from one room, in detail

For the detailed step-by-step scenarios I included, here is the full [questionnaire](https://forms.gle/RsvqniBv2tfaro7S6).

### Materials
The questionnaire included the following:

1. Background questions
2. Pre-test questions
3. Task Scenarios
4. Post-test questions
    * [System Usability Scale (SUS)](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html)
    * [Product Satisfaction Card](https://elearn.uni-sofia.bg/pluginfile.php/55103/mod_resource/content/0/Resources/Systems_Evaluation/DesirabilityToolkit.doc)
5. Demographic questions

### Procedure
First, I asked the participant about prior experiences that might have influenced the participant's test performance, such as previous exposure to the website or similar sites. Specifically, I asked the user whether they had visited the Frick Collection Virtual Tour site before. Because the participant had not used the site before, I skipped to the pre-test questions to understand the participant's initial impressions. I asked the user how easy/difficult the virtual tour interface appeared and what they thought the controls at the bottom did.

Then, I read aloud the task scenarios and invited the participant to try to complete them on the site. The participant recorded whether or not they were able to complete each task, and also provided live commentary about the features of the site.

These were the tasks the participant attempted: 

**1. Finding a specific painting by name**
   - In this task, I wanted to test whether the user would find it difficult to jump between the search function on the main site and the virtual tour itself
   - Since the paintings are catalogued in a database separate from the virtual tour window, I anticipated some difficulty here

**2. "Walking" through the virtual museum by traveling from room to room**
   - I chose this task because I imagine that users who seek out a virtual museum experience expect some degree of immersion or similarity to a real life museum visit
   - I foresaw some difficulties here due to the poor integration of the museum map and the virtual rooms

**3. Viewing multiple paintings from one room, in detail**
   - Another test of immersion
   - I wanted to test how the user reacted to having to open a new tab to view a painting, as well as not being able to view it in full screen

After the user completed the tasks, I assessed user perceptions of the difficulty and efficiency of tasks they just completed using 7-point Likert scales (Very easy/quick to Very difficult/long). 

Additionally, I included standard measures of satisfaction (System Usability Scale) and perceptions (Product Satisfaction Card). The System Usability Scale includes items to assess ease of use and learnability. The Product Satisfaction Card allows the user to endorse 5 top adjectives that they associate with the website.

At the end of the session, I asked questions to gather demographic information about the user. I placed these at the end of the questionnaire to prevent these questions from influencing responses on the usability test (stereotype threat, bias).

## What I learned
* Users' evaluation of the severity of problems differed quite a bit from my own.
* Users don't always take the path that you would have taken to solve the problem.
* Users are willing to tolerate some more global usability issues (needing to open separate tabs to view artwork in detail), but may not tolerate more acute, smaller usability issues (not being able to scroll while hovering over specific content).
* Some questions ("How likely are you to do this task?") were ambiguous and needed my verbal clarification. These could be better articulated on the questionnaire.
* Because of the unique purpose of this website (entertainment/art consumption), I should have included more measures of subjective sastisfaction that are specific to museum visits. For example, I could have included more qualitative measures of how well the site highlighted the art or how likely they would prefer this format over real-life museum visits. 
* Next time, I would also include measures of how much they would value a website that allows them to do virtual museum tours. This could inform me about how useful the website is.
